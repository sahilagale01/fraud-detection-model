{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# FRAUDULENT TRANSACTION DETECTION MODEL\n",
        "\n",
        "# --------------------------------------------------------------------------\n",
        "# 1. SETUP AND DATA SOURCE\n",
        "# --------------------------------------------------------------------------\n",
        "# This script builds a machine learning model to detect fraudulent credit card transactions.\n",
        "# We will use the \"Credit Card Fraud Detection Dataset 2023\" from Kaggle.\n",
        "#\n",
        "# --- SOURCE ---\n",
        "# Dataset: Credit Card Fraud Detection Dataset 2023\n",
        "# Kaggle URL: https://www.kaggle.com/datasets/nelgiriyewithana/credit-card-fraud-detection-dataset-2023\n",
        "#\n",
        "# --- INSTRUCTIONS ---\n",
        "# 1. Download the dataset from the URL above.\n",
        "# 2. Unzip the file and place 'creditcard_2023.csv' in the same directory as this script.\n",
        "# 3. Run the script.\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "import joblib # Add this import at the top of your script\n",
        "\n",
        "print(\"Fraud Detection Model Script - Initialized\")\n",
        "\n",
        "# --------------------------------------------------------------------------\n",
        "# --------------------------------------------------------------------------\n",
        "# 2. DATA LOADING AND PREPARATION (Corrected)\n",
        "# --------------------------------------------------------------------------\n",
        "try:\n",
        "    # Load the dataset using pandas\n",
        "    file_path = 'creditcard_2023.csv'\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    print(f\"Successfully loaded dataset: {file_path}\")\n",
        "    print(\"----------------------------------------\")\n",
        "\n",
        "    # --- Data Cleaning and Preprocessing ---\n",
        "    # The previous output showed missing values. We must remove them before proceeding.\n",
        "    # .dropna() removes rows with NaN values. 'inplace=True' modifies the DataFrame directly.\n",
        "    print(f\"Original shape of dataset: {df.shape}\")\n",
        "    df.dropna(inplace=True)\n",
        "    print(f\"Shape after dropping missing values: {df.shape}\\n\")\n",
        "\n",
        "    # We will drop the 'id' column as it's a unique identifier and not useful for prediction.\n",
        "    if 'id' in df.columns:\n",
        "        df = df.drop('id', axis=1)\n",
        "        print(\"Dropped 'id' column.\")\n",
        "\n",
        "    # The 'Amount' feature has a wide range of values. Scaling it helps the model\n",
        "    # learn more effectively. We use StandardScaler.\n",
        "    scaler = StandardScaler()\n",
        "    df['scaled_amount'] = scaler.fit_transform(df['Amount'].values.reshape(-1, 1))\n",
        "\n",
        "    # Drop the original 'Amount' column.\n",
        "    df = df.drop('Amount', axis=1)\n",
        "    print(\"'Amount' feature has been scaled and original column dropped.\")\n",
        "    print(\"----------------------------------------\")\n",
        "\n",
        "    # The script now continues to Section 3...\n",
        "    # --------------------------------------------------------------------------\n",
        "    # 3. MODEL TRAINING\n",
        "    # --------------------------------------------------------------------------\n",
        "\n",
        "    # --- Feature and Target Selection ---\n",
        "    # 'X' contains all the features the model will learn from.\n",
        "    # 'y' is the target variable we want to predict (0 for legitimate, 1 for fraud).\n",
        "    X = df.drop('Class', axis=1)\n",
        "    y = df['Class']\n",
        "\n",
        "    print(f\"Target Class Distribution:\\n{y.value_counts(normalize=True) * 100}\")\n",
        "    print(\"\\nSplitting data into training (80%) and testing (20%) sets...\")\n",
        "\n",
        "    # --- Splitting the Data ---\n",
        "    # We split the data into a training set (to train the model) and a testing set\n",
        "    # (to evaluate its performance on unseen data).\n",
        "    # The 'stratify=y' argument ensures that the proportion of fraud/non-fraud\n",
        "    # transactions is the same in both the train and test sets. This is crucial\n",
        "    # for imbalanced datasets.\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42, stratify=y\n",
        "    )\n",
        "    print(\"Data splitting complete.\")\n",
        "\n",
        "    # --- Training the Logistic Regression Model ---\n",
        "    # Logistic Regression is a good, interpretable baseline model for classification.\n",
        "    print(\"\\nTraining the Logistic Regression model...\")\n",
        "    model = LogisticRegression(max_iter=1000) # Increased max_iter for convergence\n",
        "    model.fit(X_train, y_train)\n",
        "    print(\"Model training complete.\")\n",
        "    print(\"----------------------------------------\")\n",
        "\n",
        "    # --------------------------------------------------------------------------\n",
        "    # 4. MODEL EVALUATION\n",
        "    # --------------------------------------------------------------------------\n",
        "    print(\"\\nEvaluating the model on the test set...\")\n",
        "\n",
        "    # Make predictions on the test data\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # --- Performance Metrics ---\n",
        "    # Accuracy: Overall percentage of correct predictions.\n",
        "    # Confusion Matrix: A table showing correct vs. incorrect predictions for each class.\n",
        "    # Classification Report: Shows precision, recall, and F1-score for each class.\n",
        "    #   - Precision: Of all transactions predicted as fraud, how many were actually fraud?\n",
        "    #   - Recall: Of all actual fraud transactions, how many did the model correctly identify?\n",
        "\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "    class_report = classification_report(y_test, y_pred, target_names=['Legitimate (0)', 'Fraud (1)'])\n",
        "\n",
        "    print(f\"\\nModel Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "    print(\"\\nConfusion Matrix:\")\n",
        "    print(conf_matrix)\n",
        "    # The matrix is structured as:\n",
        "    # [[True Negative, False Positive],\n",
        "    #  [False Negative, True Positive]]\n",
        "\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(class_report)\n",
        "    print(\"----------------------------------------\")\n",
        "    print(\"Script finished.\")\n",
        "\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: The file 'creditcard_2023.csv' was not found.\")\n",
        "    print(\"Please make sure the dataset is downloaded from Kaggle and placed in the correct directory.\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred: {e}\")\n",
        "\n",
        "# --------------------------------------------------------------------------\n",
        "# 5. SAVE THE TRAINED MODEL AND SCALER\n",
        "# --------------------------------------------------------------------------\n",
        "# We save the trained model and the scaler to files.\n",
        "model_filename = 'fraud_model.joblib'\n",
        "scaler_filename = 'scaler.joblib' # Define scaler filename\n",
        "\n",
        "joblib.dump(model, model_filename)\n",
        "joblib.dump(scaler, scaler_filename) # Save the scaler\n",
        "\n",
        "print(f\"\\nModel has been saved to '{model_filename}'\")\n",
        "print(f\"Scaler has been saved to '{scaler_filename}'\") # Print message for scaler\n",
        "print(\"----------------------------------------\")\n",
        "print(\"Script finished.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUQZwam2EL1J",
        "outputId": "d0e5478a-5079-4199-aa46-302ac8a68f1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fraud Detection Model Script - Initialized\n",
            "Successfully loaded dataset: creditcard_2023.csv\n",
            "----------------------------------------\n",
            "Original shape of dataset: (227538, 31)\n",
            "Shape after dropping missing values: (227537, 31)\n",
            "\n",
            "Dropped 'id' column.\n",
            "'Amount' feature has been scaled and original column dropped.\n",
            "----------------------------------------\n",
            "Target Class Distribution:\n",
            "Class\n",
            "0.0    99.816733\n",
            "1.0     0.183267\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "Splitting data into training (80%) and testing (20%) sets...\n",
            "Data splitting complete.\n",
            "\n",
            "Training the Logistic Regression model...\n",
            "Model training complete.\n",
            "----------------------------------------\n",
            "\n",
            "Evaluating the model on the test set...\n",
            "\n",
            "Model Accuracy: 0.9990\n",
            "\n",
            "Confusion Matrix:\n",
            "[[45415    10]\n",
            " [   35    48]]\n",
            "\n",
            "Classification Report:\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "Legitimate (0)       1.00      1.00      1.00     45425\n",
            "     Fraud (1)       0.83      0.58      0.68        83\n",
            "\n",
            "      accuracy                           1.00     45508\n",
            "     macro avg       0.91      0.79      0.84     45508\n",
            "  weighted avg       1.00      1.00      1.00     45508\n",
            "\n",
            "----------------------------------------\n",
            "Script finished.\n",
            "\n",
            "Model has been saved to 'fraud_model.joblib'\n",
            "Scaler has been saved to 'scaler.joblib'\n",
            "----------------------------------------\n",
            "Script finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import joblib\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# --- 1. Load the Saved Model and Scaler ---\n",
        "# Note: You must also save and load the scaler used during training\n",
        "# to apply the *exact same* transformation to the new data.\n",
        "# For simplicity in this example, we re-create it, but saving it is best practice.\n",
        "print(\"Loading the saved fraud detection model...\")\n",
        "model = joblib.load('fraud_model.joblib')\n",
        "scaler = StandardScaler()\n",
        "print(\"Model loaded successfully.\")\n",
        "\n",
        "# --- 2. Load and Prepare Your New Dataset ---\n",
        "# Replace 'your_new_dataset.csv' with the actual file name.\n",
        "try:\n",
        "    new_data = pd.read_csv('your_new_dataset.csv')\n",
        "    print(f\"\\nSuccessfully loaded new dataset with {new_data.shape[0]} transactions.\")\n",
        "\n",
        "    # IMPORTANT: Apply the EXACT same preprocessing steps as the training data\n",
        "    # a. Drop the 'id' column if it exists\n",
        "    if 'id' in new_data.columns:\n",
        "        new_data = new_data.drop('id', axis=1)\n",
        "\n",
        "    # b. Store the original 'Amount' for reference before scaling\n",
        "    original_amounts = new_data['Amount'].copy()\n",
        "\n",
        "    # c. Scale the 'Amount' feature\n",
        "    new_data['scaled_amount'] = scaler.fit_transform(new_data['Amount'].values.reshape(-1, 1))\n",
        "    new_data = new_data.drop('Amount', axis=1)\n",
        "\n",
        "    # d. Ensure column order is the same as the training data\n",
        "    # (Excluding the original 'Class' column, which we want to predict)\n",
        "    # This step is crucial if your new CSV has columns in a different order.\n",
        "    features_for_prediction = new_data.drop('Class', axis=1, errors='ignore') # ignore error if 'Class' doesn't exist\n",
        "\n",
        "\n",
        "    # --- 3. Make Predictions ---\n",
        "    print(\"\\nMaking predictions on the new data...\")\n",
        "    predictions = model.predict(features_for_prediction)\n",
        "\n",
        "\n",
        "    # --- 4. Display the Results ---\n",
        "    print(\"Predictions complete.\")\n",
        "    results_df = pd.DataFrame({\n",
        "        'Original_Amount': original_amounts,\n",
        "        'Prediction': predictions\n",
        "    })\n",
        "\n",
        "    # Map prediction from 0/1 to meaningful labels\n",
        "    results_df['Result'] = results_df['Prediction'].apply(lambda x: 'Fraud' if x == 1 else 'Legitimate')\n",
        "\n",
        "    print(\"\\n--- Prediction Results ---\")\n",
        "    print(results_df)\n",
        "\n",
        "    print(\"\\n--- Summary ---\")\n",
        "    print(results_df['Result'].value_counts())\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"\\nError: Make sure 'your_new_dataset.csv' is in the same directory.\")\n",
        "except Exception as e:\n",
        "    print(f\"\\nAn error occurred: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-MdkBmMLkGvo",
        "outputId": "b423ccc3-2a32-45a2-a2fb-bab81ed071b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading the saved fraud detection model...\n",
            "Model loaded successfully.\n",
            "\n",
            "Successfully loaded new dataset with 6 transactions.\n",
            "\n",
            "An error occurred: 'Amount'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import joblib\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# --- 1. Load the Saved Model and Scaler ---\n",
        "print(\"Loading the saved fraud detection model and scaler...\")\n",
        "model = joblib.load('fraud_model.joblib')\n",
        "scaler = joblib.load('scaler.joblib') # <-- LOAD the saved scaler\n",
        "print(\"Model and scaler loaded successfully.\")\n",
        "\n",
        "\n",
        "# --- 2. Load and Prepare Your New Dataset ---\n",
        "try:\n",
        "    # Read the CSV file, specifying that the second row is the header (skip the first row)\n",
        "    new_data = pd.read_csv('your_new_dataset.csv', skiprows=1, header=0) # <-- ADDED skiprows=1\n",
        "    print(f\"\\nSuccessfully loaded new dataset with {new_data.shape[0]} transactions.\")\n",
        "\n",
        "    # --- DIAGNOSTIC STEP: Print column names ---\n",
        "    print(\"\\nColumns in the loaded new dataset:\")\n",
        "    print(new_data.columns.tolist())\n",
        "    print(\"------------------------------------\")\n",
        "    # --- END DIAGNOSTIC STEP ---\n",
        "\n",
        "\n",
        "    # IMPORTANT: Apply the EXACT same preprocessing steps as the training data\n",
        "    # a. Drop the 'id' column if it exists\n",
        "    if 'id' in new_data.columns:\n",
        "        new_data = new_data.drop('id', axis=1)\n",
        "\n",
        "    # --- Specific Handling for 'Amount' column ---\n",
        "    if 'Amount' not in new_data.columns:\n",
        "        raise ValueError(\"The 'Amount' column is missing from the new dataset.\")\n",
        "\n",
        "    # Ensure 'Amount' column contains numeric data and handle non-numeric/missing values early\n",
        "    new_data['Amount'] = pd.to_numeric(new_data['Amount'], errors='coerce')\n",
        "    # Drop rows where 'Amount' could not be converted to numeric (NaNs)\n",
        "    new_data.dropna(subset=['Amount'], inplace=True)\n",
        "\n",
        "    # b. Store the original 'Amount' for reference before scaling\n",
        "    # Now this copy happens AFTER ensuring 'Amount' is numeric and non-null\n",
        "    original_amounts = new_data['Amount'].copy()\n",
        "\n",
        "\n",
        "    # c. Scale the 'Amount' feature using the LOADED scaler\n",
        "    # Use .transform() ONLY, NOT .fit_transform()\n",
        "    # This applies the original scaling from the full training dataset.\n",
        "    new_data['scaled_amount'] = scaler.transform(new_data['Amount'].values.reshape(-1, 1))\n",
        "\n",
        "    # Drop the original 'Amount' column.\n",
        "    new_data = new_data.drop('Amount', axis=1)\n",
        "\n",
        "\n",
        "    # d. Ensure column order is the same as the training data\n",
        "    # (Excluding the original 'Class' column, which we want to predict)\n",
        "    # This step is crucial if your new CSV has columns in a different order.\n",
        "    # We'll align the columns of new_data with the columns used for training the model (X_train)\n",
        "    # Get the list of columns from the training data, excluding the target variable 'Class'\n",
        "    # Assuming the training dataframe 'df' was created in a previous cell and is available\n",
        "    # If 'df' is not available, you would need to save and load the list of feature columns.\n",
        "\n",
        "    # Re-order columns to match the training data features.\n",
        "    # This assumes X is available from the previous run.\n",
        "    # Get the columns from the training features DataFrame\n",
        "    training_columns = X.columns.tolist()\n",
        "\n",
        "    # Ensure all necessary columns from training_columns are in new_data (except 'Class' which is not in new data)\n",
        "    # and convert relevant columns to numeric\n",
        "    for col in training_columns:\n",
        "        if col in new_data.columns and new_data[col].dtype == 'object':\n",
        "             new_data[col] = pd.to_numeric(new_data[col], errors='coerce')\n",
        "\n",
        "    # Drop rows with non-numeric values in feature columns AFTER converting\n",
        "    new_data.dropna(subset=training_columns, inplace=True)\n",
        "\n",
        "\n",
        "    # Reindex the new data DataFrame to match the training columns\n",
        "    # 'errors='ignore'' will skip columns in training_columns not found in new_data\n",
        "    # 'fill_value=0' can be used if you want to fill missing columns with a default value\n",
        "    features_for_prediction = new_data.reindex(columns=training_columns, fill_value=0)\n",
        "\n",
        "    # Ensure the data types match between training and prediction\n",
        "    # Only attempt to cast if the column exists in features_for_prediction\n",
        "    for col in training_columns:\n",
        "        if col in features_for_prediction.columns and col in X.columns: # Check if col exists in both\n",
        "            features_for_prediction[col] = features_for_prediction[col].astype(X[col].dtype)\n",
        "\n",
        "\n",
        "    # --- 3. Make Predictions ---\n",
        "    # Check if features_for_prediction is empty after cleaning\n",
        "    if features_for_prediction.empty:\n",
        "        print(\"\\nNo valid data remaining after preprocessing. Cannot make predictions.\")\n",
        "    else:\n",
        "        print(\"\\nMaking predictions on the new data...\")\n",
        "        predictions = model.predict(features_for_prediction)\n",
        "\n",
        "\n",
        "        # --- 4. Display the Results ---\n",
        "        print(\"Predictions complete.\")\n",
        "        results_df = pd.DataFrame({\n",
        "            'Original_Amount': original_amounts.loc[features_for_prediction.index], # Align original amounts with cleaned data\n",
        "            'Prediction': predictions\n",
        "        })\n",
        "\n",
        "        # Map prediction from 0/1 to meaningful labels\n",
        "        results_df['Result'] = results_df['Prediction'].apply(lambda x: 'Fraud' if x == 1 else 'Legitimate')\n",
        "\n",
        "        print(\"\\n--- Prediction Results ---\")\n",
        "        print(results_df)\n",
        "\n",
        "        print(\"\\n--- Summary ---\")\n",
        "        print(results_df['Result'].value_counts())\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"\\nError: Make sure 'your_new_dataset.csv' is in the same directory.\")\n",
        "except ValueError as ve: # Catch the specific Value Error for missing 'Amount'\n",
        "     print(f\"\\nData Error: {ve}\")\n",
        "except Exception as e:\n",
        "    print(f\"\\nAn unexpected error occurred: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQ-DNmeyjtcz",
        "outputId": "86410e93-ae2e-4c58-b17a-5b8f979fae3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading the saved fraud detection model and scaler...\n",
            "Model and scaler loaded successfully.\n",
            "\n",
            "Successfully loaded new dataset with 5 transactions.\n",
            "\n",
            "Columns in the loaded new dataset:\n",
            "['id', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount', 'Class']\n",
            "------------------------------------\n",
            "\n",
            "Making predictions on the new data...\n",
            "Predictions complete.\n",
            "\n",
            "--- Prediction Results ---\n",
            "   Original_Amount  Prediction      Result\n",
            "0           150.75         1.0       Fraud\n",
            "1          8500.00         1.0       Fraud\n",
            "2            45.50         0.0  Legitimate\n",
            "3          1230.20         1.0       Fraud\n",
            "4            89.99         0.0  Legitimate\n",
            "\n",
            "--- Summary ---\n",
            "Result\n",
            "Fraud         3\n",
            "Legitimate    2\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    }
  ]
}